{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"demo.ipynb","provenance":[],"collapsed_sections":["gjpbsDvhi0QV","WJejpOpmjDy1","XRc5FUgIjTWs","HuoEUuGx5tki","FMeAqJSS_v8D","U3WC_d01s13X","S6ugJyBl_AMl","N_oEv6uC_GBY","B74NbJON_PK_"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xnkcrFAHtCgq"},"source":["## Set Environment"]},{"cell_type":"markdown","metadata":{"id":"INM8Z0mUvlt5"},"source":["Before everything, **Edit-Notebook settings-GPU-save**"]},{"cell_type":"code","metadata":{"id":"jOjMpshfchZl","outputId":"614bafd3-da1e-45d9-fea1-395f4803b3f5","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","4/5wFJbMW-vOmhBAh_z1hT-sLCBoml7p0BENZWv6oJRTndXeHdy18cazQ\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"IL81thBndcFo","executionInfo":{"status":"ok","timestamp":1604306401454,"user_tz":-480,"elapsed":932,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"f654c40e-925d-4063-8b61-b4389bdbd7d2","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd '/content/gdrive/My Drive/'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gjpbsDvhi0QV"},"source":["### Part 1 (Can be skipped!!!!)\n","If you already has **your own** ssh public and private key pair, then **skip** this part"]},{"cell_type":"code","metadata":{"id":"eLzFkNyZeJFb"},"source":["!mkdir ssh_keys"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kMPMsScFeL0c","executionInfo":{"status":"ok","timestamp":1604229052194,"user_tz":-480,"elapsed":1149,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"4bf3a48f-c008-4f44-e5aa-18fa4d6d0a3f","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ls -a"],"execution_count":null,"outputs":[{"output_type":"stream","text":["set_environment.ipynb  .ssh\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"oAITH0Bbdx7s"},"source":["!ssh-keygen\n","\n","# follow this:\n","# /content/gdrive/My Drive/ssh_keys/id_rsa\n","# Enter\n","# Enter"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nKiq6ApDe3pn"},"source":["Download SSH public key: /content/gdrive/My Drive/ssh_keys/id_rsa.pub"]},{"cell_type":"markdown","metadata":{"id":"WJejpOpmjDy1"},"source":["### Part 2 (Can be skipped!!!!)\n","If the key pair **does not exist** in github settings, go to github.com, log in to your account, go to settings-SSH and GPG keys, create a new SSH key, copy the key and paste."]},{"cell_type":"markdown","metadata":{"id":"XRc5FUgIjTWs"},"source":["### Part 3\n","**Copy your key pair to /root/ and Set git config**"]},{"cell_type":"code","metadata":{"id":"RVRrXsQtGVKK","executionInfo":{"status":"ok","timestamp":1604306406080,"user_tz":-480,"elapsed":1086,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"40db8853-1268-41ba-9198-d01b47ad5ea0","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd '/root'\n","!mkdir '.ssh'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/root\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xlEJnQUqkidx"},"source":["!cp -r '/content/gdrive/My Drive/ssh_keys/.' '/root/.ssh'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qwkOrz2vkrn_","executionInfo":{"status":"ok","timestamp":1604306407867,"user_tz":-480,"elapsed":2866,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"ad3844b6-5bd0-4704-e51d-4e2c9bb28af3","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd '/root/.ssh'\n","!ls -a"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/root/.ssh\n",".  ..  id_rsa  id_rsa.pub\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8w0qCB9ODr8a"},"source":["**Enter your own information here**"]},{"cell_type":"code","metadata":{"id":"GtxJfRbIhd8q"},"source":["# !git config --global user.email \"addy@xyz.com\"\n","# !git config --global user.name \"your_name\"\n","\n","!git config --global user.email \"mattcoldwater@gmail.com\"\n","!git config --global user.name \"mattcoldwater\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MMTBfm_Mh8qX","executionInfo":{"status":"ok","timestamp":1604306408420,"user_tz":-480,"elapsed":3414,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"d0a54f81-6a1d-428c-fed0-623c60537f8c","colab":{"base_uri":"https://localhost:8080/"}},"source":["!ssh -T -o StrictHostKeyChecking=no git@github.com"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Warning: Permanently added 'github.com,140.82.112.3' (RSA) to the list of known hosts.\r\n","Hi mattcoldwater! You've successfully authenticated, but GitHub does not provide shell access.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"HuoEUuGx5tki"},"source":["### Part 4\n","run this block to commit and push (change commit comment!)"]},{"cell_type":"markdown","metadata":{"id":"4sAmaXg77ehz"},"source":["connect：git remote add origin(can be changed) branch_Name(default:main) url\n","\n","git remote -v to check\n","\n","nomrally pull first：git pull origin master\n","\n","specially: git merge origin/master --allow-unrelated-histories\n","\n","Then you need to deal with some conflicts\n","\n","push：git push -u origin master"]},{"cell_type":"code","metadata":{"id":"3dZkcVsN6fyI","executionInfo":{"status":"ok","timestamp":1604303798193,"user_tz":-480,"elapsed":976,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"da25567a-3ff2-458b-8aa1-1f4e2da41528","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd '/content/gdrive/My Drive/CSE527-Project/RNN_visualization/'\n","!ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/CSE527-Project/RNN_visualization\n","cuda_IndRNN_onlyrecurrent.py  Indrnn_action_train.py   __pycache__\n","data_reader_ntu.py\t      IndRNN_onlyrecurrent.py  README.md\n","Indrnn_action_network.py      opts.py\t\t       skeleton_to_numpy.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mr3U44EW7lcl","executionInfo":{"status":"ok","timestamp":1604303800256,"user_tz":-480,"elapsed":1085,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"237a6b9a-deaa-44b7-ec70-540778be10cb","colab":{"base_uri":"https://localhost:8080/"}},"source":["# !git remote add origin master\n","!git remote -v"],"execution_count":null,"outputs":[{"output_type":"stream","text":["origin\tgit@github.com:mattcoldwater/RNN_visualization.git (fetch)\n","origin\tgit@github.com:mattcoldwater/RNN_visualization.git (push)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"V-5qhj-850a_"},"source":["!git add ."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D6AZaa3X8GuS","executionInfo":{"status":"ok","timestamp":1604303805081,"user_tz":-480,"elapsed":1115,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"bbafd883-779d-4bec-93a8-757f967ecae7","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git commit -m \"add gitinore\""],"execution_count":null,"outputs":[{"output_type":"stream","text":["On branch main\n","Your branch is ahead of 'origin/main' by 1 commit.\n","  (use \"git push\" to publish your local commits)\n","\n","nothing to commit, working tree clean\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tQMjhRIW8FZO","executionInfo":{"status":"ok","timestamp":1604303808913,"user_tz":-480,"elapsed":1394,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"bd29701a-d7be-4cce-9f78-74c0be9300e9","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git pull origin main"],"execution_count":null,"outputs":[{"output_type":"stream","text":["From github.com:mattcoldwater/RNN_visualization\n"," * branch            main       -> FETCH_HEAD\n","Already up to date.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7NrmxZY-6RJq","executionInfo":{"status":"ok","timestamp":1604303814330,"user_tz":-480,"elapsed":4168,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"c05cf3cc-8f01-4c97-a36e-1f7ac6fbbf58","colab":{"base_uri":"https://localhost:8080/"}},"source":["!git push -u origin main"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Counting objects: 3, done.\n","Delta compression using up to 2 threads.\n","Compressing objects:  50% (1/2)   \rCompressing objects: 100% (2/2)   \rCompressing objects: 100% (2/2), done.\n","Writing objects:  33% (1/3)   \rWriting objects:  66% (2/3)   \rWriting objects: 100% (3/3)   \rWriting objects: 100% (3/3), 294 bytes | 147.00 KiB/s, done.\n","Total 3 (delta 1), reused 0 (delta 0)\n","remote: Resolving deltas:   0% (0/1)\u001b[K\rremote: Resolving deltas: 100% (1/1)\u001b[K\rremote: Resolving deltas: 100% (1/1), completed with 1 local object.\u001b[K\n","To github.com:mattcoldwater/RNN_visualization.git\n","   d20b170..5e6412c  main -> main\n","Branch 'main' set up to track remote branch 'main' from 'origin'.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4_fgAv5lvOw-"},"source":["## Demo"]},{"cell_type":"markdown","metadata":{"id":"FMeAqJSS_v8D"},"source":["### preparation"]},{"cell_type":"code","metadata":{"id":"CxDvSfAU2xL0","executionInfo":{"status":"ok","timestamp":1604306430978,"user_tz":-480,"elapsed":1121,"user":{"displayName":"Coldwater Matt","photoUrl":"","userId":"00624643367407395142"}},"outputId":"e79ce505-82bd-49e3-d885-4dedce0d6e0c","colab":{"base_uri":"https://localhost:8080/"}},"source":["%cd '/content/gdrive/My Drive/CSE527-Project/RNN_visualization'\n","!ls -a"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/CSE527-Project/RNN_visualization\n","cuda_IndRNN_onlyrecurrent.py  .gitignore\t\topts.py\n","Data\t\t\t      Indrnn_action_network.py\t__pycache__\n","data_reader_ntu.py\t      Indrnn_action_train.py\tREADME.md\n",".git\t\t\t      IndRNN_onlyrecurrent.py\tskeleton_to_numpy.py\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"0mbiWGjF4jau"},"source":["cuda_IndRNN_onlyrecurrent.py  Indrnn_action_train.py   IndRNN_onlyrecurrent.py\n","Indrnn_action_network.py"]},{"cell_type":"code","metadata":{"id":"bbsr5C8S2cVo"},"source":["!cat README.md"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U3WC_d01s13X"},"source":["### import packages"]},{"cell_type":"code","metadata":{"id":"eUSVmutQszYg"},"source":["import sys\n","import argparse\n","import os\n","import time\n","import numpy as np\n","import copy\n","import math\n","import csv\n","import glob\n","# import opts \n","from __future__ import division\n","from __future__ import print_function\n","\n","# from cupy.cuda import function\n","# from pynvrtc.compiler import Program\n","# from collections import namedtuple\n","\n","from sklearn.utils import shuffle\n","from sklearn.model_selection import train_test_split\n","\n","import torch\n","import torch.nn as nn\n","from torch.autograd import Variable\n","import torch.nn.functional as F\n","from torch.nn import Parameter\n","from torch.autograd import Function\n","import torch.nn.init as weight_init\n","from torch.nn.utils.rnn import pad_packed_sequence as unpack\n","from torch.nn.utils.rnn import pack_padded_sequence as pack"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FavHkbCF_2MP"},"source":["### get numpy skeleton data"]},{"cell_type":"code","metadata":{"id":"H-jBQjMp35EC"},"source":["## to do"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6ugJyBl_AMl"},"source":["### RNN_onlyrecurrent"]},{"cell_type":"code","metadata":{"id":"3b85AQnY-oxo"},"source":["class IndRNNCell_onlyrecurrent(nn.Module):\n","    r\"\"\"An IndRNN cell with ReLU non-linearity. This is only the recurrent part where the input is already processed with w_{ih} * x + b_{ih}.\n","\n","    .. math::\n","        input=w_{ih} * x + b_{ih}\n","        h' = \\relu(input +  w_{hh} (*) h)\n","    With (*) being element-wise vector multiplication.\n","\n","    Args:\n","        hidden_size: The number of features in the hidden state h\n","\n","    Inputs: input, hidden\n","        - **input** (batch, input_size): tensor containing input features\n","        - **hidden** (batch, hidden_size): tensor containing the initial hidden\n","          state for each element in the batch.\n","\n","    Outputs: h'\n","        - **h'** (batch, hidden_size): tensor containing the next hidden state\n","          for each element in the batch\n","    \"\"\"\n","\n","    def __init__(self, hidden_size, \n","                 hidden_max_abs=None, recurrent_init=None):\n","        super(IndRNNCell_onlyrecurrent, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.recurrent_init = recurrent_init\n","        self.weight_hh = Parameter(torch.Tensor(hidden_size))            \n","        self.reset_parameters()\n","\n","    def reset_parameters(self):\n","        for name, weight in self.named_parameters():\n","            if \"weight_hh\" in name:\n","                if self.recurrent_init is None:\n","                    nn.init.uniform(weight, a=0, b=1)\n","                else:\n","                    self.recurrent_init(weight)\n","\n","    def forward(self, input, hx):\n","        return F.relu(input + hx * self.weight_hh.unsqueeze(0).expand(hx.size(0), len(self.weight_hh)))\n","\n","\n","class IndRNN_onlyrecurrent(nn.Module):\n","    r\"\"\"Applies an IndRNN with `ReLU` non-linearity to an input sequence. \n","    This is only the recurrent part where the input is already processed with w_{ih} * x + b_{ih}.\n","\n","\n","    For each element in the input sequence, each layer computes the following\n","    function:\n","\n","    .. math::\n","\n","        h_t = \\relu(input_t +  w_{hh} (*) h_{(t-1)})\n","\n","    where :math:`h_t` is the hidden state at time `t`, and :math:`input_t`\n","    is the input at time `t`. (*) is element-wise multiplication.\n","\n","    Args:\n","        hidden_size: The number of features in the hidden state `h`\n","        batch_first: If ``True``, then the input and output tensors are provided\n","            as `(batch, seq, feature)`\n","\n","    Inputs: input, h_0\n","        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features\n","          of the input sequence. The input can also be a packed variable length\n","          sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`\n","          or :func:`torch.nn.utils.rnn.pack_sequence`\n","          for details.\n","        - **h_0** of shape `(num_directions, batch, hidden_size)`: tensor\n","          containing the initial hidden state for each element in the batch.\n","          Defaults to zero if not provided.\n","\n","    Outputs: output, h_n\n","        - **output** of shape `(seq_len, batch, hidden_size * num_directions)`: tensor\n","          containing the output features (`h_k`) from the last layer of the RNN,\n","          for each `k`.  If a :class:`torch.nn.utils.rnn.PackedSequence` has\n","          been given as the input, the output will also be a packed sequence.\n","        - **h_n** (num_directions, batch, hidden_size): tensor\n","          containing the hidden state for `k = seq_len`.\n","    \"\"\"\n","\n","    def __init__(self, hidden_size, \n","                 batch_first=False, bidirectional=False, recurrent_inits=None,\n","                 **kwargs):\n","        super(IndRNN_onlyrecurrent, self).__init__()\n","        self.hidden_size = hidden_size\n","        self.batch_first = batch_first\n","        self.bidirectional = bidirectional\n","\n","        num_directions = 2 if self.bidirectional else 1\n","\n","        if batch_first:\n","            self.time_index = 1\n","            self.batch_index = 0\n","        else:\n","            self.time_index = 0\n","            self.batch_index = 1\n","\n","        cells = []\n","        directions = []\n","        if recurrent_inits is not None:\n","            kwargs[\"recurrent_init\"] = recurrent_inits\n","        for dir in range(num_directions):\n","            directions.append(IndRNNCell_onlyrecurrent(hidden_size, **kwargs))\n","        self.cells = nn.ModuleList(directions)\n","\n","        h0 = torch.zeros(hidden_size * num_directions)\n","        self.register_buffer('h0', torch.autograd.Variable(h0))\n","\n","    def forward(self, x, hidden=None):\n","        time_index = self.time_index\n","        batch_index = self.batch_index\n","        num_directions = 2 if self.bidirectional else 1\n","        hidden_init = self.h0.unsqueeze(0).expand(\n","            x.size(batch_index),\n","            self.hidden_size * num_directions).contiguous()\n","\n","        x_n = []\n","        for dir, cell in enumerate(self.cells):\n","            hx_cell = hidden_init[\n","                :, self.hidden_size * dir: self.hidden_size * (dir + 1)]\n","            outputs = []\n","            hiddens = []\n","            x_T = torch.unbind(x, time_index)\n","            if dir == 1:\n","                x_T = reversed(x_T)\n","            for x_t in x_T:\n","                hx_cell = cell(x_t, hx_cell)\n","                outputs.append(hx_cell)\n","            if dir == 1:\n","                outputs = outputs[::-1]\n","            x_cell = torch.stack(outputs, time_index)\n","            x_n.append(x_cell)\n","            hiddens.append(hx_cell)\n","        x = torch.cat(x_n, -1)\n","        return x.squeeze(2), torch.cat(hiddens, -1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N_oEv6uC_GBY"},"source":["### complete RNN"]},{"cell_type":"code","metadata":{"id":"eMgYot3m-90B"},"source":["class Batch_norm_step(nn.Module):\n","    def __init__(self,  hidden_size,seq_len):\n","        super(Batch_norm_step, self).__init__()\n","        self.hidden_size = hidden_size\n","        \n","        self.max_time_step=seq_len\n","        self.bn = nn.BatchNorm1d(hidden_size) \n","\n","    def forward(self, x):\n","        x=x.permute(1,2,0)\n","        x= self.bn(x.clone())\n","        x=x.permute(2,0,1)\n","        return x\n","class Dropout_overtime(torch.autograd.Function):\n","  @staticmethod\n","  def forward(ctx, input, p=0.5,training=False):\n","    output = input.clone()\n","    noise = input.data.new(input.size(-2),input.size(-1))  #torch.ones_like(input[0])\n","    if training:            \n","      noise.bernoulli_(1 - p).div_(1 - p)\n","      noise = noise.unsqueeze(0).expand_as(input)\n","      output.mul_(noise)\n","    ctx.save_for_backward(noise)\n","    ctx.training=training\n","    return output\n","  @staticmethod\n","  def backward(ctx, grad_output):\n","    noise,=ctx.saved_tensors\n","    if ctx.training:\n","      return grad_output.mul(noise),None,None\n","    else:\n","      return grad_output,None,None\n","dropout_overtime=Dropout_overtime.apply\n","\n","import argparse\n","import opts     \n","parser = argparse.ArgumentParser(description='pytorch action')\n","opts.train_opts(parser)\n","args = parser.parse_args()\n","MAG=args.MAG\n","U_bound=np.power(10,(np.log10(MAG)/args.seq_len))\n","U_lowbound=np.power(10,(np.log10(1.0/MAG)/args.seq_len))  \n","  \n","class stackedIndRNN_encoder(nn.Module):\n","    def __init__(self, input_size, outputclass, isCuda):\n","        super(stackedIndRNN_encoder, self).__init__()        \n","        hidden_size=args.hidden_size\n","        \n","        self.isCuda = isCuda\n","            \n","        self.DIs=nn.ModuleList()\n","        denseinput=nn.Linear(input_size*3, hidden_size, bias=True)\n","        self.DIs.append(denseinput)\n","        for x in range(args.num_layers - 1):\n","            denseinput = nn.Linear(hidden_size, hidden_size, bias=True)\n","            self.DIs.append(denseinput)                \n","        \n","        self.BNs = nn.ModuleList()\n","        for x in range(args.num_layers):\n","            bn = Batch_norm_step(hidden_size,args.seq_len)\n","            self.BNs.append(bn)                      \n","  \n","        self.RNNs = nn.ModuleList()\n","        rnn = IndRNN(hidden_size=hidden_size) #IndRNN\n","        self.RNNs.append(rnn)  \n","        for x in range(args.num_layers-1):\n","            rnn = IndRNN(hidden_size=hidden_size) #IndRNN\n","            self.RNNs.append(rnn)         \n","            \n","        self.lastfc = nn.Linear(hidden_size, outputclass, bias=True)\n","        self.init_weights()\n","\n","    def init_weights(self):\n","      for name, param in self.named_parameters():\n","        if 'weight_hh' in name:\n","          param.data.uniform_(0,U_bound)          \n","        if 'RNNs.'+str(args.num_layers-1)+'.weight_hh' in name:\n","          param.data.uniform_(U_lowbound,U_bound)    \n","        if 'DIs' in name and 'weight' in name:\n","          param.data.uniform_(-args.ini_in2hid,args.ini_in2hid)               \n","        if 'bns' in name and 'weight' in name:\n","          param.data.fill_(1)      \n","        if 'bias' in name:\n","          param.data.fill_(0.0)              \n","    def forward(self, input):\n","        all_output = []\n","        rnnoutputs={}\n","        hidden_x={}               \n","        seq_len, batch_size, indim,_=input.size()\n","        #print(seq_len, batch_size, indim)\n","             \n","        input=input.view(seq_len,batch_size,3*indim)                  \n","        #print(input.shape)\n","        for x in range(1,len(self.RNNs)+1):\n","          if self.isCuda:\n","              hidden_x['hidden%d'%x]=Variable(torch.zeros(1,batch_size,args.hidden_size).cuda())\n","          else:\n","              hidden_x['hidden%d'%x]=Variable(torch.zeros(1,batch_size,args.hidden_size))\n","                            \n","        rnnoutputs['rnnlayer0']=input\n","        for x in range(1,len(self.RNNs)+1):\n","          rnnoutputs['rnnlayer%d'%(x-1)]=rnnoutputs['rnnlayer%d'%(x-1)].contiguous().view(seq_len*batch_size,-1)\n","          rnnoutputs['rnnlayer%d'%(x-1)]=self.DIs[x-1](rnnoutputs['rnnlayer%d'%(x-1)])   \n","          rnnoutputs['rnnlayer%d'%(x-1)]=rnnoutputs['rnnlayer%d'%(x-1)].view(seq_len,batch_size,-1)  \n","          #rnnoutputs['rnnlayer%d'%x]= self.RNNs[x-1](rnnoutputs['rnnlayer%d'%(x-1)], hidden_x['hidden%d'%x])        \n","          if self.isCuda:\n","              rnn_x_out = self.RNNs[x-1](rnnoutputs['rnnlayer%d'%(x-1)], hidden_x['hidden%d'%x])\n","          else:\n","              rnn_x_out, rnn_h_out= self.RNNs[x-1](rnnoutputs['rnnlayer%d'%(x-1)], hidden_x['hidden%d'%x])\n","              hidden_x['hidden%d'%x] = rnn_h_out\n","\n","          rnnoutputs['rnnlayer%d'%x] = rnn_x_out\n","\n","          rnnoutputs['rnnlayer%d'%x]=self.BNs[x-1](rnnoutputs['rnnlayer%d'%x])     \n","          if args.dropout>0:\n","            rnnoutputs['rnnlayer%d'%x]= dropout_overtime(rnnoutputs['rnnlayer%d'%x],args.dropout,self.training) \n","        temp=rnnoutputs['rnnlayer%d'%len(self.RNNs)][-1]\n","        output = self.lastfc(temp)\n","        return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B74NbJON_PK_"},"source":["### train"]},{"cell_type":"code","metadata":{"id":"saAqnOuD_Q6T"},"source":["# Set the random seed manually for reproducibility.\n","seed=100\n","torch.manual_seed(seed)\n","CUDA = False\n","if torch.cuda.is_available():\n","  torch.cuda.manual_seed(seed)\n","  CUDA = True\n","else:\n","  print(\"WARNING: CUDA not available\")\n","  CUDA = False\n","\n","import opts     \n","parser = argparse.ArgumentParser(description='pytorch action')\n","opts.train_opts(parser)\n","args = parser.parse_args()\n","print(args)\n","\n","import Indrnn_action_network\n","\n","batch_size = args.batch_size\n","print('BATCH_SIZE', batch_size)\n","seq_len=args.seq_len\n","outputclass=60\n","indim=50\n","gradientclip_value=10\n","U_bound=Indrnn_action_network.U_bound\n","\n","\n","\n","model = Indrnn_action_network.stackedIndRNN_encoder(indim, outputclass, CUDA)  \n","if CUDA:\n","    model.cuda()\n","criterion = nn.CrossEntropyLoss()\n","\n","#Adam with lr 2e-4 works fine.\n","learning_rate=args.lr\n","if args.use_weightdecay_nohiddenW:\n","  param_decay=[]\n","  param_nodecay=[]\n","  for name, param in model.named_parameters():\n","    if 'weight_hh' in name or 'bias' in name:\n","      param_nodecay.append(param)      \n","      #print('parameters no weight decay: ',name)          \n","    else:\n","      param_decay.append(param)      \n","      #print('parameters with weight decay: ',name)          \n","\n","  if args.opti=='sgd':\n","    optimizer = torch.optim.SGD([\n","            {'params': param_nodecay},\n","            {'params': param_decay, 'weight_decay': args.decayfactor}\n","        ], lr=learning_rate,momentum=0.9,nesterov=True)   \n","  else:                \n","    optimizer = torch.optim.Adam([\n","            {'params': param_nodecay},\n","            {'params': param_decay, 'weight_decay': args.decayfactor}\n","        ], lr=learning_rate) \n","else:  \n","  if args.opti=='sgd':   \n","    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate,momentum=0.9,nesterov=True)\n","  else:                      \n","    optimizer=torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","if args.test_CV:\n","  train_datasets='train_CV_ntus'\n","  test_dataset='test_CV_ntus'\n","else:\n","  train_datasets='train_ntus'\n","  test_dataset='test_ntus'\n","  \n","#from data_reader_numpy_witheval import DataHandler_train,DataHandler_eval  \n","#from data_reader_numpy_test import DataHandler as testDataHandler\n","\n","from data_reader_ntu import initialize_data_handlers\n","\n","dh_train, dh_eval, dh_test = initialize_data_handlers(batch_size, seq_len)\n","\n","#dh_train = DataHandler_train(batch_size,seq_len)\n","#dh_eval = DataHandler_eval(batch_size,seq_len)\n","#dh_test= testDataHandler(batch_size,seq_len)\n","\n","num_train_batches=int(np.ceil(dh_train.GetDatasetSize()/(batch_size+0.0)))\n","num_eval_batches=int(np.ceil(dh_eval.GetDatasetSize()/(batch_size+0.0)))\n","num_test_batches=int(np.ceil(dh_test.GetDatasetSize()/(batch_size+0.0)))\n","\n","\n","def train(num_train_batches):\n","  model.train()\n","  tacc=0\n","  count=0\n","  start_time = time.time()\n","  for batchi in range(0,num_train_batches):\n","    inputs,targets=dh_train.GetBatch()\n","    inputs=inputs.transpose(1,0,2,3)\n","    \n","    if CUDA:\n","        inputs=Variable(torch.from_numpy(inputs).cuda(), requires_grad=True).float()\n","        targets=Variable(torch.from_numpy(np.int64(targets)).cuda(), requires_grad=False)\n","    else:\n","        inputs=Variable(torch.from_numpy(inputs), requires_grad=True).float()\n","        targets=Variable(torch.from_numpy(np.int64(targets)), requires_grad=False)\n","\n","    model.zero_grad()\n","    if args.constrain_U:\n","      clip_weight(model,U_bound)\n","    output=model(inputs)\n","    loss = criterion(output, targets)\n","\n","    pred = output.data.max(1)[1] # get the index of the max log-probability\n","    accuracy = pred.eq(targets.data).cpu().sum().numpy()/(0.0+targets.size(0))      \n","          \n","    loss.backward()\n","    clip_gradient(model,gradientclip_value)\n","    optimizer.step()\n","    \n","    tacc=tacc+accuracy#loss.data.cpu().numpy()#accuracy\n","    count+=1\n","  elapsed = time.time() - start_time\n","  print (\"training accuracy: \", tacc/(count+0.0)  )\n","  #print ('time per batch: ', elapsed/num_train_batches)\n","  \n","def set_bn_train(m):\n","    classname = m.__class__.__name__\n","    if classname.find('BatchNorm') != -1:\n","      m.train()       \n","def eval(dh,num_batches,use_bn_trainstat=False):\n","  model.eval()\n","  if use_bn_trainstat:\n","    model.apply(set_bn_train)\n","  tacc=0\n","  count=0  \n","  start_time = time.time()\n","  while(1):  \n","    inputs,targets=dh.GetBatch()\n","    inputs=inputs.transpose(1,0,2,3)\n","    if CUDA:\n","        inputs=Variable(torch.from_numpy(inputs).cuda()).float()\n","        targets=Variable(torch.from_numpy(np.int64(targets)).cuda())\n","    else: \n","        inputs=Variable(torch.from_numpy(inputs)).float()\n","        targets=Variable(torch.from_numpy(np.int64(targets)))\n","    output=model(inputs)\n","    pred = output.data.max(1)[1] # get the index of the max log-probability\n","    accuracy = pred.eq(targets.data).cpu().sum().numpy()        \n","    tacc+=accuracy\n","    count+=1\n","    if count==num_batches*args.eval_fold:\n","      break\n","  elapsed = time.time() - start_time\n","  print (\"eval accuracy: \", tacc/(count*targets.data.size(0)+0.0)  )\n","  #print ('eval time per batch: ', elapsed/(count+0.0))\n","  return tacc/(count*targets.data.size(0)+0.0)\n","\n","\n","def test(dh,num_batches,use_bn_trainstat=False):\n","  model.eval()\n","  if use_bn_trainstat:\n","    model.apply(set_bn_train)\n","  tacc=0\n","  count=0  \n","  start_time = time.time()\n","  total_testdata=dh_test.GetDatasetSize()  \n","  total_ave_acc = None\n","  test_labels = None\n","  while(1):  \n","    inputs,targets=dh.GetBatch()\n","    inputs=inputs.transpose(1,0,2,3)\n","    if test_labels is None:\n","        test_labels = targets\n","    else:\n","        test_labels=np.concatenate(\n","                [test_labels, targets], \n","                axis=0,\n","                )\n","    if CUDA:\n","        inputs=Variable(torch.from_numpy(inputs).cuda()).float()\n","        targets=Variable(torch.from_numpy(np.int64(targets)).cuda())\n","    else:\n","        inputs=Variable(torch.from_numpy(inputs)).float()\n","        targets=Variable(torch.from_numpy(np.int64(targets)))\n","        \n","    output=model(inputs)\n","    pred = output.data.max(1)[1] # get the index of the max log-probability\n","    accuracy = pred.eq(targets.data).cpu().sum().numpy()    \n","    \n","    if not total_ave_acc is None :\n","        total_ave_acc = np.concatenate(\n","                [total_ave_acc, output.data.cpu().numpy()]\n","                , axis=0,\n","                )\n","    else:\n","        total_ave_acc = output.data.cpu().numpy()\n","    \n","    tacc+=accuracy\n","    count+=1\n","    if count==num_batches:\n","      break    \n","  top = np.argmax(total_ave_acc, axis=-1)\n","  eval_acc=np.mean(np.equal(top, test_labels))    \n","  elapsed = time.time() - start_time\n","  print (\"test accuracy: \",  eval_acc)\n","  #print ('test time per batch: ', elapsed/(count+0.0))\n","  return eval_acc\n","\n","def clip_gradient(model, clip):\n","    for p in model.parameters():\n","        p.grad.data.clamp_(-clip,clip)\n","        #print(p.size(),p.grad.data)\n","\n","def adjust_learning_rate(optimizer, lr):\n","    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = lr     \n","\n","def clip_weight(RNNmodel, clip):\n","    for name, param in RNNmodel.named_parameters():\n","      if 'weight_hh' in name:\n","        param.data.clamp_(-clip,clip)\n","    \n","lastacc=0\n","dispFreq=20\n","patience=0\n","reduced=1\n","for batchi in range(1,10000000):\n","  for i in range(num_train_batches//dispFreq):\n","\n","    train(dispFreq)\n","  test_acc=eval(dh_eval,num_eval_batches,args.use_bneval)\n","  \n","  if (test_acc >lastacc):\n","    model_clone = copy.deepcopy(model.state_dict())   \n","    opti_clone = copy.deepcopy(optimizer.state_dict()) \n","    lastacc=test_acc\n","    patience=0\n","  elif patience>int(args.pThre/reduced+0.5):\n","    reduced=reduced*2\n","    print ('learning rate',learning_rate)\n","    model.load_state_dict(model_clone)\n","    optimizer.load_state_dict(opti_clone)\n","    patience=0\n","    learning_rate=learning_rate*0.1\n","    adjust_learning_rate(optimizer,learning_rate)     \n","    if learning_rate<args.end_rate:\n","      break  \n","    test_acc=test(dh_test,num_test_batches)     \n"," \n","  else:\n","    patience+=1 \n","    \n","test_acc=test(dh_test,num_test_batches)  \n","test_acc=test(dh_test,num_test_batches,True)     \n","save_name='indrnn_action_model' \n","with open(save_name, 'wb') as f:\n","    torch.save(model, f)"],"execution_count":null,"outputs":[]}]}